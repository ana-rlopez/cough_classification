{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Research references:\n",
    "#1) Dry/wet cough classification: https://link.springer.com/article/10.1007/s10439-013-0741-6\n",
    "#2) Pneumonia classification: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987276\n",
    "#3) https://espace.library.uq.edu.au/data/UQ_344963/s41943203_phd_submission.pdf?Expires=1585601065&Key-Pair-Id=APKAJKNBJ4MJBJNC6NLQ&Signature=Lnpf6wT8rkozSh9av7U9nGuC7WAH6KuI2Cj3Y7G366gkGlh8D-Ie1Kc~TyBAUu~uMsVltleJcSv3p6TCm6HdFnhpyoTgLcYh6eFfvQwIUqbk1Bf4JZldgB~BDKUOwY1G0pA-HoKjvIAu3avO98SMO35upakm9OEBByd4nC9aXsjKRThd6bTpq1qIuuD9gh1l5FaM6hNRB0c2lCf4Q3adx7C3FW0NMwdWhcuF45A9f~dO3zTWWSQamoo5Otc-PHMMt96TetNcML~jy9ghgJeCPY6DJLUIwQAt03fENBluS~TjTJ17WD~n51xiRofb94fEJHoRHh0d-430LLwr7BX4IA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#import pywt #wavelets\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.utils import mediainfo\n",
    "from pydub.playback import play\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sn\n",
    "import python_speech_features as spe_feats\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import lfilter\n",
    "import librosa\n",
    "import pysptk\n",
    "import math\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "#settings\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_s=[]\n",
    "all_label=[]\n",
    "all_id=[]\n",
    "all_fs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/YT_set/edited_wavs/edit_Wet Throat Infection Cough-tfc5cXiXMDc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Wet Throat Infection Cough-tfc5cXiXMDc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Heavy cold and sore throat coughing.-NaOVmYoIjbs-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Heavy cold and sore throat coughing.-NaOVmYoIjbs-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing 2-AQOeIVbhFm4-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing 2-AQOeIVbhFm4-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Allergy Coughing-7Ez5Wc_esBg-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Allergy Coughing-7Ez5Wc_esBg-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Wheezing Chest and Wet Cough 2-5905FxXz9dI-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Wheezing Chest and Wet Cough 2-5905FxXz9dI-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 34 coughing up crap again-rkF_uMizqoc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 34 coughing up crap again-rkF_uMizqoc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 30 Chesty and wet cough-d2wkdrScerU-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 30 Chesty and wet cough-d2wkdrScerU-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_DIST_Coughing 79-h2FLCKMcEX0-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_DIST_Coughing 79-h2FLCKMcEX0-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing Woman Sound - Woman Cough Sound Effect-zjd4HrJbc8o-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing Woman Sound - Woman Cough Sound Effect-zjd4HrJbc8o-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_# 61 morning phlegmy cough...again-qfpJg179YNk-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 61 morning phlegmy cough...again-qfpJg179YNk-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 31 night wet cough-Dc_aoUCqw2E-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 31 night wet cough-Dc_aoUCqw2E-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_DIST_Another Girl Coughing-iYxUHA-Pwsk-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_DIST_Another Girl Coughing-iYxUHA-Pwsk-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Afternoon Cough-6LK6yHtIung-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Afternoon Cough-6LK6yHtIung-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 14 - After work-1UDFq2InljM-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 14 - After work-1UDFq2InljM-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Residual Phlegmy Morning Coughing and Gagging-TK4CveeCWfY-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Residual Phlegmy Morning Coughing and Gagging-TK4CveeCWfY-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Cough Around the Clock!-4k0ziD0j5BI-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Cough Around the Clock!-4k0ziD0j5BI-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Early Morning Cough-XrpB4DTNQZw-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Early Morning Cough-XrpB4DTNQZw-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Man Coughing Sound - Wet Cough Sound Effect-q6WsoL3J8U8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Man Coughing Sound - Wet Cough Sound Effect-q6WsoL3J8U8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Single wet cough-CTSLdNxN1cc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Single wet cough-CTSLdNxN1cc-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 51-LkxvBb2VXbs-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 51-LkxvBb2VXbs-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Wet coughing-0QQxKN-KC1U-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Wet coughing-0QQxKN-KC1U-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 77-2Mw-s5jnqXU-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 77-2Mw-s5jnqXU-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Coughing Fit in the Afternoon.-A5s2ZgwQ1VM-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Coughing Fit in the Afternoon.-A5s2ZgwQ1VM-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 46-dg-I9j76-t8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 46-dg-I9j76-t8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Male bronchitis cough-IzPMbIll3LE-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Male bronchitis cough-IzPMbIll3LE-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing 3-tZtJaS2ZtME-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing 3-tZtJaS2ZtME-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Mid-morning Winter Coughing Fit-h-GtQfDCoaE-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Mid-morning Winter Coughing Fit-h-GtQfDCoaE-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_# 60 coughing still (deep and wet cough)-jxYNLCYTwZQ-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 60 coughing still (deep and wet cough)-jxYNLCYTwZQ-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_#64 coughing, allergies, singing lungs-CsDXlt7Ei1c-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_#64 coughing, allergies, singing lungs-CsDXlt7Ei1c-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_My deep wet cough-De4HdyocTHY-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_My deep wet cough-De4HdyocTHY-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 55 gaggy wet cough-ct3tHDfNKiQ-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_# 55 gaggy wet cough-ct3tHDfNKiQ-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_More Allergy Coughing-NfKZNt25L-Q-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_More Allergy Coughing-NfKZNt25L-Q-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_November cold (wet coughing)-DYfjPnty2Ho-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_November cold (wet coughing)-DYfjPnty2Ho-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing.-u2KMBD5-oCg-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Spring Cold Coughing.-u2KMBD5-oCg-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 60-diuuEXKzNB8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Coughing 60-diuuEXKzNB8-Wet.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Morning Cough turns Chesty and Barking.-ekqLlw-Xe68-Dry.wav\n",
      "data/YT_set/edited_wavs/edit_Dry Morning Cough turns Chesty and Barking.-ekqLlw-Xe68-Dry.wav\n"
     ]
    }
   ],
   "source": [
    "#Read wav data set\n",
    "\n",
    "if config.featExtr_skip is False:\n",
    "    #print(\"Readings wavs...\")\n",
    "\n",
    "    #only list files in FOLDER_PATH directory\n",
    "    wav_files = [f for f in os.listdir(config.FOLDER_PATH) if os.path.isfile(os.path.join(config.FOLDER_PATH, f))]\n",
    "    for file_name in wav_files:\n",
    "    \n",
    "        fname_noExt = os.path.splitext(file_name)[0] #file name without extension\n",
    "    \n",
    "        #full path file name\n",
    "        full_fname = config.FOLDER_PATH+file_name\n",
    "        #print(full_fname)\n",
    "    \n",
    "        # load audio\n",
    "        s = AudioSegment.from_wav(full_fname)\n",
    "        print(full_fname)\n",
    "        all_s.append(s)\n",
    "        #sampling rate:\n",
    "        info = mediainfo(full_fname)\n",
    "        fs = float(info['sample_rate'])\n",
    "        all_fs.append(fs)\n",
    "    \n",
    "        #get ID of recording\n",
    "        ID = fname_noExt.split('-')[-2] #for the current type of naming\n",
    "        #print(file_name)\n",
    "        #print(ID)\n",
    "        all_id.append(ID)\n",
    "    \n",
    "        #get label\n",
    "        label = fname_noExt.split('-')[-1] #for the current type of naming\n",
    "        #print(label)\n",
    "        all_label.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listening to some of the audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.featExtr_skip is False:\n",
    "    np.where(np.array(all_label)=='Dry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.featExtr_skip is False:\n",
    "    np.where(np.array(all_label)=='Wet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.featExtr_skip is False:\n",
    "    s=all_s[15]\n",
    "    s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute RMS value of a signal and return it (in dB scale)\n",
    "#seems not to work?\n",
    "def get_RMS(s):\n",
    "    s_rms = np.sqrt(np.mean(np.power(s,2)))\n",
    "    #convert to dB scale\n",
    "    #s_db = 20*np.log10(s_rms/1.0)\n",
    "    return s_rms\n",
    "\n",
    "#seems not to work either\n",
    "#RMS-based normalization of a signal, based on a target level (in dB)\n",
    "def RMS_normalization(s, dB_targ):\n",
    "    \n",
    "    #desired level is converted to linear scale\n",
    "    rms_targ = 10**(dB_targ/20.0)\n",
    "    \n",
    "    #compute scaling factor\n",
    "    scale = rms_targ/get_RMS(s)\n",
    "    \n",
    "    #scale amplitude of input signal\n",
    "    scaled_s = scale*s\n",
    "    \n",
    "    return scaled_s\n",
    "\n",
    "def match_target_amplitude(audioSegment_sound, target_dBFS):\n",
    "    dBFS_diff = target_dBFS - audioSegment_sound.dBFS\n",
    "    return audioSegment_sound.apply_gain(dBFS_diff)\n",
    "\n",
    "#Apply pre-emphasis (high-pass) filter\n",
    "def apply_preEmph(x):\n",
    "    x_filt = lfilter([1., -0.97], 1, x)\n",
    "    return x_filt\n",
    "        \n",
    "#Obtain autocorrelation\n",
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    return result[int((result.size+1)/2):] #Note: other people use re.size/2:, but this does not work for me \n",
    "                                   # TODO: check consistency in other computers\n",
    "\n",
    "#Compute zero-crossing rate\n",
    "def get_zcr(x):\n",
    "    zcr = (((x[:-1] * x[1:]) < 0).sum())/(len(x)-1)\n",
    "    return zcr\n",
    "\n",
    "#Compute log-energy\n",
    "def get_logEnergy(x):\n",
    "    logEnergy = np.log10( ( (np.power(x,2)).sum()/len(x) ) + config.eps)  \n",
    "    return logEnergy\n",
    "\n",
    "#Estimate fundamental frequency (F0)\n",
    "def get_F0(x,fs):\n",
    "    #autocorrelation-based method to extract F0\n",
    "    xcorr_arr = autocorr(x)\n",
    "    \n",
    "    #looking for F0 in the frequency interval 50-500Hz, but we search in time domain\n",
    "    min_ms = round(fs/500)\n",
    "    max_ms = round(fs/50)\n",
    "    \n",
    "    xcorr_slot = xcorr_arr[max_ms+1:2*max_ms+1]\n",
    "    xcorr_slot = xcorr_slot[min_ms:max_ms]\n",
    "    t0 = np.argmax(xcorr_slot)\n",
    "    F0 = fs/(min_ms+t0-1)\n",
    "    return F0\n",
    "\n",
    "#Estimate formants\n",
    "def get_formants(x, lp_order, nr_formants):\n",
    "    \n",
    "    #compute lp coefficients\n",
    "    a = librosa.lpc(x, lp_ord)\n",
    "    \n",
    "\n",
    "    #get roots from lp coefficients\n",
    "    rts = np.roots(a)\n",
    "    rts = [r for r in rts if np.imag(r) >= 0]\n",
    "\n",
    "    #get angles\n",
    "    angz = np.arctan2(np.imag(rts), np.real(rts))\n",
    "\n",
    "    #get formant frequencies\n",
    "    formants = sorted(angz * (fs_targ / (2 * math.pi)))\n",
    "    \n",
    "    return formants[0:nr_formants]\n",
    "\n",
    "def get_entropy(x, type='shannon'):\n",
    "    #default shannon entropy since this is the one used by the phd thesis\n",
    "    \n",
    "    base = {'shannon' : 2., 'natural' : math.exp(1), 'hartley' : 10.}\n",
    "    N = len(x)\n",
    "\n",
    "    if N <= 1:\n",
    "        return 0\n",
    "\n",
    "    value,counts = np.unique(x, return_counts=True)\n",
    "    probs = counts / N\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "\n",
    "    ent = 0. #initialization\n",
    "\n",
    "    # compute entropy\n",
    "    for i in probs:\n",
    "        ent -= i * math.log(i+config.eps, base[type])\n",
    "    \n",
    "    return ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(x,fs,feats_df,lp_ord,ID,label):\n",
    "#Extract features from signal x (identified as ID), and concatenate them to dataframe feats_df\n",
    "#Features' reference: (see Appendix)\n",
    "#[1]https://link.springer.com/article/10.1007/s10439-013-0741-6\n",
    "#[2]https://espace.library.uq.edu.au/data/UQ_344963/s41943203_phd_submission.pdf?dsi_version=c5434db897ab74b192ca295a9eeca041&Expires=1585086202&Key-Pair-Id=APKAJKNBJ4MJBJNC6NLQ&Signature=c8k8DmG~KIxg0ToTO8rebm2MzHneCzJGkjSFRB7BYTEQ-MHXEr0ocHmISrldP3hFf9qmeiL11ezyefcNeRVeKIQ9PVjOl9pn7rXWcjA1o2voPn1VnDd8n7G2cT31apdj0LNMclhlXRPnCsGD66qDRqa3d-xaqqXhEqU73aw3ZgBgroO213MfJOqFhJxxXo2QEia0bSlDRTeX9KhSczFK-IFTPC6GwFL2L04por8pQRI3HF7E3f26O9zp9OhkwxSU9qfJah20WxZLA4PxREdv7JGoVBinR6T0mTcIaQi~B4IzYjSPSsTTADMNk5znVYIvSqgtMT~DY~qwlfq4SRdFjQ__\n",
    "  \n",
    "    \n",
    "    #do features in a frame-basis\n",
    "    x_frames = spe_feats.sigproc.framesig(x,config.frame_len,config.frame_step,config.win_func) #DOUBT: should I use window or not?\n",
    "                                                                        #at least for formant estimation i should\n",
    "\n",
    "    nr_frames = x_frames.shape[0]\n",
    "    #print(nr_frames)\n",
    "        \n",
    "    #0)Wavelets #TODO\n",
    "    \n",
    "    #DOUBT: if log-energy feature is included, should I also include the first mfcc coefficient (c0) ?\n",
    "    #1)mfcc\n",
    "    mfcc_feat = spe_feats.mfcc(x,fs, winlen=config.frame_len_s,winstep=config.frame_step_s,    numcep=config.cep_num,winfunc=config.win_func)\n",
    "   # print('MFCC Features --- ', mfcc_feat)\n",
    "    mfcc_feat = zscore(mfcc_feat, axis=1,ddof=1)\n",
    "  #  print('MFCC Features new --- ', mfcc_feat)\n",
    "    #deltas to capture \n",
    "    mfcc_delta_feat = spe_feats.delta(mfcc_feat,1) #mfcc_delta_feat = np.subtract(mfcc_feat[:-1], mfcc_feat[1:]) #same\n",
    "    mfcc_deltadelta_feat = spe_feats.delta(mfcc_delta_feat,1)          \n",
    "    \n",
    "    #2)zero-crossing rate\n",
    "    zcr_feat = np.apply_along_axis(get_zcr, 1, x_frames)\n",
    "    \n",
    "    #3)Formant frequencies\n",
    "    #using LP-coeffcs-based method\n",
    "    #formant_feat = np.apply_along_axis(get_formants, 1, x_frames, lp_ord, nr_formants)\n",
    "    \n",
    "    #Note: for the moment, it seems some frames are ill-conditioned for lp computing,\n",
    "    #current solution - we skip those and fill with NaN values\n",
    "    formants_feat= np.empty((nr_frames,4))\n",
    "    formants_feat[:] = np.nan\n",
    "    \n",
    "    for i_frame in range(0,nr_frames):\n",
    "        try: \n",
    "            formants_feat[i_frame] = get_formants(x_frames[i_frame], config.lp_ord, config.nr_formants)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #4)Log-energy\n",
    "    logEnergy_feat =  np.apply_along_axis(get_logEnergy, 1, x_frames)\n",
    "    \n",
    "    #5)Pitch (F0)\n",
    "    F0_feat =  np.apply_along_axis(get_F0, 1, x_frames,fs)\n",
    "    \n",
    "    #TODO: compute also F0 with pysptk (a python wrapper for SPTK library), it probably gives better results\n",
    "    #https://github.com/r9y9/pysptk/blob/master\n",
    "        \t#F0_feat = pysptk.rapt(x.astype(np.float32), fs=fs, hopsize=frame_step, min=50, max=500, ,voice_bias=0.0 ,otype=\\\"f0\\\")\n",
    "    \n",
    "    #Compare the values between swipe and rapt \n",
    "    #F0_feat = pysptk.swipe(x.astype(np.float64), fs=fs,hopsize = config.frame_step, min=50, max=500, otype=\"f0\")\n",
    "    \n",
    "    F0_feat = pysptk.rapt(x.astype(np.float32), fs=fs,hopsize = config.frame_step, min=50, max=500, otype=\"f0\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #right frame size???\n",
    "#Change the window size from 450 to 40 to 100\n",
    "# Keep swipe , change min to 50 and max - 500\n",
    "#EXample pysptk.swipe(x.astype(np.float64), fs=fs, hopsize=80, min=60, max=200, otype=\"f0\")\n",
    "\n",
    "    \n",
    "    #6)Kurtosis\n",
    "    kurt_feat =  np.apply_along_axis(kurtosis, 1, x_frames)\n",
    "    \n",
    "    #7)Bispectrum Score (BGS)\n",
    "    #TODO: see PhD thesis for more info on this feature\n",
    "    \n",
    "    #8)Non-Gaussianity Score (NGS)\n",
    "    #TODO: see PhD thesis for more info on this feature\n",
    "   \n",
    "    #9) Adding skewness as measure of non-gaussianity (not in paper)\n",
    "    skew_feat =  np.apply_along_axis(skew, 1, x_frames)\n",
    "    \n",
    "    #DOUBT: 10) Shannon entropy GETTING -inf in all cases, WHY??? Don't include until fixed\n",
    "    entropy_feat = np.apply_along_axis(get_entropy, 1, x_frames)\n",
    "\n",
    "    \n",
    "    #TODO: add small value in all entries, this may fix the problem\n",
    "    \n",
    "    mfcc_cols = ['mfcc_%s' % s for s in range(0,config.cep_num)]\n",
    "    mfcc_delta_cols = ['mfcc_d%s' % s for s in range(0,config.cep_num)]\n",
    "    mfcc_deltadelta_cols = ['mfcc_dd%s' % s for s in range(0,config.cep_num)]\n",
    "    formants_cols = ['F%s' % s for s in range(1,config.nr_formants+1)]\n",
    "          \n",
    "    feats_segment = pd.concat([pd.DataFrame({'Id': ID, 'kurt': kurt_feat, 'logEnergy': logEnergy_feat,\n",
    "                                                 'zcr': zcr_feat, 'F0': F0_feat,\n",
    "                                                 'skewness': skew_feat, 'label': label, 'entropy':entropy_feat}),\n",
    "                               pd.DataFrame(mfcc_feat,columns=mfcc_cols), \n",
    "                               pd.DataFrame(mfcc_delta_feat,columns=mfcc_delta_cols), \n",
    "                               pd.DataFrame(mfcc_deltadelta_feat,columns=mfcc_deltadelta_cols), \n",
    "                            pd.DataFrame(formants_feat,columns=formants_cols)],axis=1)\n",
    "    \n",
    "    #print(nr_frames)\n",
    "    feats_df = feats_df.append(feats_segment,ignore_index=True, sort=False)\n",
    "    \n",
    "    return feats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_Step(all_s,all_id,all_label):\n",
    "    \n",
    "    import config\n",
    "    import pydub\n",
    "    \n",
    "    #initialize data frame of features:\n",
    "    feats = pd.DataFrame([])\n",
    "\n",
    "    for s, ID, label in zip(all_s,all_id,all_label):\n",
    "\n",
    "            #Pre-processing of the signals:\n",
    "\n",
    "            ## 0 ) Resampling to target sampling frequency:\n",
    "            s = s.set_frame_rate(config.fs_targ)\n",
    "            fs= config.fs_targ\n",
    "\n",
    "            ## 1)\n",
    "            if config.norm_skip is False:\n",
    "                s=match_target_amplitude(s, config.dB_targ)\n",
    "                #print(s.rms)\n",
    "\n",
    "            ## 2) Segmentation of cough streams (silence-based)\n",
    "            #min_silence_len in ms, silence_thresh in dB\n",
    "            s_segments = split_on_silence (s, min_silence_len = 600, silence_thresh =s.dBFS-10)\n",
    "\n",
    "            #checks that segmentation and removal of silence is OK\n",
    "            #print(len(s_segments))\n",
    "            #play(s)\n",
    "            #input(\"Press Enter to continue...\")               \n",
    "            #for i in range(len(s_segments)):\n",
    "            #    play(s_segments[i])\n",
    "            #    input(\"Press Enter to continue...\")               \n",
    "\n",
    "            ## 3) Convert s_segments to numpy array format\n",
    "            AudioSegment2numpy_arr = lambda x: np.asarray(x.get_array_of_samples())\n",
    "            s_segments_np = list(map(AudioSegment2numpy_arr, s_segments))\n",
    "\n",
    "\n",
    "            ## 4) Pre-emphasis filtering on each segment\n",
    "            if config.HPF_skip is False:\n",
    "               # print('High-pass filtering...')       \n",
    "                preEmph_filtering = lambda x: apply_preEmph(x)\n",
    "                s_segments_filt = list(map(preEmph_filtering, s_segments_np))\n",
    "            else:\n",
    "                s_segments_filt = s_segments_np\n",
    "\n",
    "            #print('Computing features...')\n",
    "            #Feature extraction for each segment\n",
    "\n",
    "            #(lambda function doesn't work )\n",
    "            #feat_extr_step = lambda x, fs, feats_df, lp_ord, ID: feature_extraction(x,fs,feats_df,lp_ord,ID)\n",
    "            #feats = feat_extr_step(s_segments_filt,fs,feats,lp_ord,ID)\n",
    "            for idx, seg_i in enumerate(s_segments_filt):\n",
    "                #print('\\tSegment %d' % idx)\n",
    "                #print(\"feats --- \", feats)\n",
    "                feats = feature_extraction(seg_i,fs,feats,config.lp_ord,ID,label)\n",
    "                \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13410\n"
     ]
    }
   ],
   "source": [
    "#import featureExtractionFunctions as feat\n",
    "\n",
    "if config.featExtr_skip is False:\n",
    "\n",
    "    feats = feature_extraction_Step(all_s,all_id,all_label)\n",
    "    print(len(feats))\n",
    "    \n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  (or store) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats_fname = 'feats_df.pkl'\n",
    "\n",
    "if config.featExtr_skip is False:\n",
    "    #Store feature df\n",
    "    feats.to_pickle(feats_fname)\n",
    "else:\n",
    "    #Load feature df\n",
    "    feats = pd.read_pickle(feats_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kurt</th>\n",
       "      <th>logEnergy</th>\n",
       "      <th>zcr</th>\n",
       "      <th>F0</th>\n",
       "      <th>skewness</th>\n",
       "      <th>entropy</th>\n",
       "      <th>mfcc_0</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_dd7</th>\n",
       "      <th>mfcc_dd8</th>\n",
       "      <th>mfcc_dd9</th>\n",
       "      <th>mfcc_dd10</th>\n",
       "      <th>mfcc_dd11</th>\n",
       "      <th>mfcc_dd12</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "      <td>13410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.709398</td>\n",
       "      <td>4.258418</td>\n",
       "      <td>0.681179</td>\n",
       "      <td>30.297604</td>\n",
       "      <td>-0.008360</td>\n",
       "      <td>8.568503</td>\n",
       "      <td>1.309688</td>\n",
       "      <td>-2.144157</td>\n",
       "      <td>0.189047</td>\n",
       "      <td>-0.589077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>-0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.851946</td>\n",
       "      <td>2.183823</td>\n",
       "      <td>0.165611</td>\n",
       "      <td>73.206673</td>\n",
       "      <td>0.345128</td>\n",
       "      <td>0.455845</td>\n",
       "      <td>0.366096</td>\n",
       "      <td>0.601133</td>\n",
       "      <td>0.482312</td>\n",
       "      <td>0.567827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236534</td>\n",
       "      <td>0.246343</td>\n",
       "      <td>0.233121</td>\n",
       "      <td>0.230055</td>\n",
       "      <td>0.236358</td>\n",
       "      <td>0.225308</td>\n",
       "      <td>0.225308</td>\n",
       "      <td>0.225308</td>\n",
       "      <td>0.225308</td>\n",
       "      <td>0.225308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.310083</td>\n",
       "      <td>-2.388365</td>\n",
       "      <td>0.012531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.557286</td>\n",
       "      <td>0.402292</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>-3.255308</td>\n",
       "      <td>-1.056984</td>\n",
       "      <td>-2.244664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970065</td>\n",
       "      <td>-0.978169</td>\n",
       "      <td>-1.023406</td>\n",
       "      <td>-0.993991</td>\n",
       "      <td>-1.438780</td>\n",
       "      <td>-0.849354</td>\n",
       "      <td>-0.849354</td>\n",
       "      <td>-0.849354</td>\n",
       "      <td>-0.849354</td>\n",
       "      <td>-0.849354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.582898</td>\n",
       "      <td>2.329294</td>\n",
       "      <td>0.503759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.116626</td>\n",
       "      <td>8.643856</td>\n",
       "      <td>1.066769</td>\n",
       "      <td>-2.564830</td>\n",
       "      <td>-0.148222</td>\n",
       "      <td>-1.010794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146204</td>\n",
       "      <td>-0.157349</td>\n",
       "      <td>-0.154346</td>\n",
       "      <td>-0.147343</td>\n",
       "      <td>-0.152147</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>-0.146656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.466519</td>\n",
       "      <td>5.100722</td>\n",
       "      <td>0.726817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005592</td>\n",
       "      <td>8.643856</td>\n",
       "      <td>1.297403</td>\n",
       "      <td>-2.198181</td>\n",
       "      <td>0.153556</td>\n",
       "      <td>-0.626465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001471</td>\n",
       "      <td>-0.002049</td>\n",
       "      <td>-0.007214</td>\n",
       "      <td>-0.004467</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.056754</td>\n",
       "      <td>5.980990</td>\n",
       "      <td>0.817043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>8.643856</td>\n",
       "      <td>1.531373</td>\n",
       "      <td>-1.770850</td>\n",
       "      <td>0.517530</td>\n",
       "      <td>-0.195410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142314</td>\n",
       "      <td>0.154373</td>\n",
       "      <td>0.146643</td>\n",
       "      <td>0.147291</td>\n",
       "      <td>0.150341</td>\n",
       "      <td>0.141532</td>\n",
       "      <td>0.141532</td>\n",
       "      <td>0.141532</td>\n",
       "      <td>0.141532</td>\n",
       "      <td>0.141532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>86.069435</td>\n",
       "      <td>7.442851</td>\n",
       "      <td>0.987469</td>\n",
       "      <td>472.705536</td>\n",
       "      <td>5.977376</td>\n",
       "      <td>8.643856</td>\n",
       "      <td>2.666019</td>\n",
       "      <td>0.544302</td>\n",
       "      <td>1.613358</td>\n",
       "      <td>1.450469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230357</td>\n",
       "      <td>1.211892</td>\n",
       "      <td>0.951538</td>\n",
       "      <td>0.944678</td>\n",
       "      <td>1.095596</td>\n",
       "      <td>1.052831</td>\n",
       "      <td>1.052831</td>\n",
       "      <td>1.052831</td>\n",
       "      <td>1.052831</td>\n",
       "      <td>1.052831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               kurt     logEnergy           zcr            F0      skewness  \\\n",
       "count  13410.000000  13410.000000  13410.000000  13410.000000  13410.000000   \n",
       "mean       3.709398      4.258418      0.681179     30.297604     -0.008360   \n",
       "std        3.851946      2.183823      0.165611     73.206673      0.345128   \n",
       "min       -0.310083     -2.388365      0.012531      0.000000     -2.557286   \n",
       "25%        1.582898      2.329294      0.503759      0.000000     -0.116626   \n",
       "50%        2.466519      5.100722      0.726817      0.000000     -0.005592   \n",
       "75%        5.056754      5.980990      0.817043      0.000000      0.102488   \n",
       "max       86.069435      7.442851      0.987469    472.705536      5.977376   \n",
       "\n",
       "            entropy        mfcc_0        mfcc_1        mfcc_2        mfcc_3  \\\n",
       "count  13410.000000  13410.000000  13410.000000  13410.000000  13410.000000   \n",
       "mean       8.568503      1.309688     -2.144157      0.189047     -0.589077   \n",
       "std        0.455845      0.366096      0.601133      0.482312      0.567827   \n",
       "min        0.402292      0.098600     -3.255308     -1.056984     -2.244664   \n",
       "25%        8.643856      1.066769     -2.564830     -0.148222     -1.010794   \n",
       "50%        8.643856      1.297403     -2.198181      0.153556     -0.626465   \n",
       "75%        8.643856      1.531373     -1.770850      0.517530     -0.195410   \n",
       "max        8.643856      2.666019      0.544302      1.613358      1.450469   \n",
       "\n",
       "       ...      mfcc_dd7      mfcc_dd8      mfcc_dd9     mfcc_dd10  \\\n",
       "count  ...  13410.000000  13410.000000  13410.000000  13410.000000   \n",
       "mean   ...      0.000205     -0.000216     -0.000187     -0.000862   \n",
       "std    ...      0.236534      0.246343      0.233121      0.230055   \n",
       "min    ...     -0.970065     -0.978169     -1.023406     -0.993991   \n",
       "25%    ...     -0.146204     -0.157349     -0.154346     -0.147343   \n",
       "50%    ...     -0.001471     -0.002049     -0.007214     -0.004467   \n",
       "75%    ...      0.142314      0.154373      0.146643      0.147291   \n",
       "max    ...      1.230357      1.211892      0.951538      0.944678   \n",
       "\n",
       "          mfcc_dd11     mfcc_dd12            F1            F2            F3  \\\n",
       "count  13410.000000  13410.000000  13410.000000  13410.000000  13410.000000   \n",
       "mean      -0.000493     -0.000282     -0.000282     -0.000282     -0.000282   \n",
       "std        0.236358      0.225308      0.225308      0.225308      0.225308   \n",
       "min       -1.438780     -0.849354     -0.849354     -0.849354     -0.849354   \n",
       "25%       -0.152147     -0.146656     -0.146656     -0.146656     -0.146656   \n",
       "50%       -0.000214     -0.000237     -0.000237     -0.000237     -0.000237   \n",
       "75%        0.150341      0.141532      0.141532      0.141532      0.141532   \n",
       "max        1.095596      1.052831      1.052831      1.052831      1.052831   \n",
       "\n",
       "                 F4  \n",
       "count  13410.000000  \n",
       "mean      -0.000282  \n",
       "std        0.225308  \n",
       "min       -0.849354  \n",
       "25%       -0.146656  \n",
       "50%       -0.000237  \n",
       "75%        0.141532  \n",
       "max        1.052831  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.Check which columns have NaNs values\n",
    "\n",
    "#feats2 = feats.copy()\n",
    "\n",
    "#sum(feats.isna().any())\n",
    "#feats.columns[feats.isna().any()].tolist() --> We get just the ones we have inserted in formants\n",
    "feats2 = feats.interpolate(method ='pad')\n",
    "#feats2 = feats.dropna(axis=1).copy()\n",
    "#feats2.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=True)\n",
    "\n",
    "#feats2.columns[feats2.isna().any()].tolist()\n",
    "\n",
    "#feats2 = processingNaNvalues(feats)\n",
    "feats2.describe()\n",
    "#sum(feats2.isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Id  subIdx    kurt_m  logEnergy_m     zcr_m        F0_m  \\\n",
      "0     1UDFq2InljM       0  2.893929     1.531556  0.585213    0.000000   \n",
      "1     1UDFq2InljM       1  2.110001     5.906655  0.483208    0.000000   \n",
      "2     1UDFq2InljM       2  1.625414     5.475906  0.387469  180.043884   \n",
      "3     1UDFq2InljM       3  4.231469     4.748404  0.575439    0.000000   \n",
      "4     1UDFq2InljM       4  2.924504     2.394540  0.387719    0.000000   \n",
      "...           ...     ...       ...          ...       ...         ...   \n",
      "1351  zjd4HrJbc8o      26  1.422714     5.241664  0.815789  175.066574   \n",
      "1352  zjd4HrJbc8o      27  3.486760     4.458311  0.779699  204.254715   \n",
      "1353  zjd4HrJbc8o      28  3.801287     4.955278  0.698496    0.000000   \n",
      "1354  zjd4HrJbc8o      29  3.711125     4.036131  0.659398   58.807899   \n",
      "1355  zjd4HrJbc8o      30  2.353641     3.797773  0.801587  142.319473   \n",
      "\n",
      "      skewness_m  entropy_m  mfcc_0_m  mfcc_1_m  ...  mfcc_dd8_std  \\\n",
      "0       0.097264   8.637356  1.295564 -2.423573  ...      0.311908   \n",
      "1      -0.049947   8.643856  1.117438 -1.882309  ...      0.257634   \n",
      "2       0.057277   8.643856  1.036393 -1.633634  ...      0.219773   \n",
      "3       0.164693   8.643856  1.003551 -1.984741  ...      0.264036   \n",
      "4       0.168906   8.203644  1.438887 -1.269528  ...      0.208121   \n",
      "...          ...        ...       ...       ...  ...           ...   \n",
      "1351    0.028780   8.643856  1.546951 -1.008392  ...      0.141306   \n",
      "1352   -0.058847   8.643856  1.373317 -1.114529  ...      0.174663   \n",
      "1353    0.099326   8.643856  1.920542 -2.022923  ...      0.276698   \n",
      "1354   -0.011564   8.643856  1.792421 -1.673514  ...      0.204370   \n",
      "1355   -0.227250   8.643856  1.133164 -0.790117  ...      0.077802   \n",
      "\n",
      "      mfcc_dd9_std  mfcc_dd10_std  mfcc_dd11_std  mfcc_dd12_std    F1_std  \\\n",
      "0         0.269058       0.235712       0.308460       0.310438  0.310438   \n",
      "1         0.261683       0.248836       0.171738       0.269184  0.269184   \n",
      "2         0.328418       0.199821       0.354184       0.348096  0.348096   \n",
      "3         0.267163       0.167055       0.297374       0.164489  0.164489   \n",
      "4         0.212307       0.246872       0.245456       0.159637  0.159637   \n",
      "...            ...            ...            ...            ...       ...   \n",
      "1351      0.289822       0.212840       0.312339       0.174717  0.174717   \n",
      "1352      0.223764       0.154327       0.164025       0.133235  0.133235   \n",
      "1353      0.201944       0.257505       0.171536       0.189389  0.189389   \n",
      "1354      0.327641       0.167702       0.138447       0.202109  0.202109   \n",
      "1355      0.203966       0.097762       0.127935       0.150717  0.150717   \n",
      "\n",
      "        F2_std    F3_std    F4_std  label  \n",
      "0     0.310438  0.310438  0.310438    Dry  \n",
      "1     0.269184  0.269184  0.269184    Dry  \n",
      "2     0.348096  0.348096  0.348096    Dry  \n",
      "3     0.164489  0.164489  0.164489    Dry  \n",
      "4     0.159637  0.159637  0.159637    Dry  \n",
      "...        ...       ...       ...    ...  \n",
      "1351  0.174717  0.174717  0.174717    Dry  \n",
      "1352  0.133235  0.133235  0.133235    Dry  \n",
      "1353  0.189389  0.189389  0.189389    Dry  \n",
      "1354  0.202109  0.202109  0.202109    Dry  \n",
      "1355  0.150717  0.150717  0.150717    Dry  \n",
      "\n",
      "[1356 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#Make dictionary and add label column using it \n",
    "def addLabel2df(feats):\n",
    "    feats_unique = feats.drop_duplicates(subset=['Id'])\n",
    "    label_dict = dict(zip(feats_unique.Id, feats_unique.label))\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def frame_mean_std_chunk_modeling (feats2, label_dict):\n",
    "    \n",
    "    #Grouping the frames from a same recording (Id) into chunks with the same number of frames.\n",
    "    #The training of the classifier will be based on these chunks mean and standard deviation.\n",
    "\n",
    "    feats2['cum_IDidx'] = feats2.groupby('Id').cumcount()\n",
    "\n",
    "    def get_subidx(cum_Idx,batch_size):\n",
    "        #batch needs to be an integer (or float like 3.0)\n",
    "        return int(1.0*cum_Idx/batch_size)\n",
    "\n",
    "    feats2['subIdx'] = feats2.apply(lambda x: get_subidx(x['cum_IDidx'], 10), axis=1)\n",
    "    feats2 = feats2.drop(['cum_IDidx'],axis=1)\n",
    "    \n",
    "    mean_feats = feats2.groupby(['Id','subIdx']).aggregate('mean').reset_index()\n",
    "    std_feats = feats2.groupby(['Id','subIdx']).agg(lambda x: x.std(ddof=0)).reset_index() #ddof=0 to compute population std (rather than sample std)\n",
    "    keep_same = {'Id', 'subIdx'}\n",
    "    mean_feats.columns = ['{}{}'.format(c, '' if c in keep_same else '_m') for c in mean_feats.columns]\n",
    "    std_feats.columns = ['{}{}'.format(c, '' if c in keep_same else '_std') for c in std_feats.columns]\n",
    "    \n",
    "    mean_std_feats = pd.merge(mean_feats, std_feats, on=['Id','subIdx'], how='outer')\n",
    "    \n",
    "    mean_std_feats['label'] = mean_std_feats[\"Id\"].map(label_dict)\n",
    "    #mean_std_feats[['Id','label']].head(50)\n",
    "    \n",
    "    return mean_std_feats\n",
    "   \n",
    "\n",
    "#TODO: modeling of chunks using sequence models too\n",
    "\n",
    "label_dict = addLabel2df(feats2)\n",
    "mean_std_feats = frame_mean_std_chunk_modeling (feats2,label_dict)\n",
    "\n",
    "\n",
    "print(mean_std_feats)\n",
    "\n",
    "#sum(mean_std_feats.isna().any())\n",
    "#mean_std_feats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'subIdx', 'kurt_m', 'logEnergy_m', 'zcr_m', 'F0_m', 'skewness_m',\n",
       "       'entropy_m', 'mfcc_0_m', 'mfcc_1_m',\n",
       "       ...\n",
       "       'mfcc_dd8_std', 'mfcc_dd9_std', 'mfcc_dd10_std', 'mfcc_dd11_std',\n",
       "       'mfcc_dd12_std', 'F1_std', 'F2_std', 'F3_std', 'F4_std', 'label'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std_feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1356, 101)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc_0_m</th>\n",
       "      <th>mfcc_1_m</th>\n",
       "      <th>mfcc_2_m</th>\n",
       "      <th>mfcc_3_m</th>\n",
       "      <th>mfcc_4_m</th>\n",
       "      <th>mfcc_5_m</th>\n",
       "      <th>mfcc_6_m</th>\n",
       "      <th>mfcc_7_m</th>\n",
       "      <th>mfcc_8_m</th>\n",
       "      <th>mfcc_9_m</th>\n",
       "      <th>mfcc_10_m</th>\n",
       "      <th>mfcc_11_m</th>\n",
       "      <th>mfcc_12_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.295564</td>\n",
       "      <td>-2.423573</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>-0.342158</td>\n",
       "      <td>-0.296728</td>\n",
       "      <td>0.188851</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>-0.080217</td>\n",
       "      <td>0.081012</td>\n",
       "      <td>-0.160599</td>\n",
       "      <td>0.060458</td>\n",
       "      <td>0.642605</td>\n",
       "      <td>0.897848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mfcc_0_m  mfcc_1_m  mfcc_2_m  mfcc_3_m  mfcc_4_m  mfcc_5_m  mfcc_6_m  \\\n",
       "0  1.295564 -2.423573  0.142089 -0.342158 -0.296728  0.188851 -0.005153   \n",
       "\n",
       "   mfcc_7_m  mfcc_8_m  mfcc_9_m  mfcc_10_m  mfcc_11_m  mfcc_12_m  \n",
       "0 -0.080217  0.081012 -0.160599   0.060458   0.642605   0.897848  "
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Get feature set, labels, and recording IDs\n",
    "#X_train = mean_std_feats.drop(['label','Id','subIdx'], 1).copy()\n",
    "X_train = mean_std_feats.iloc[:,8:21]\n",
    "y_train =  mean_std_feats['label'].copy()\n",
    "\n",
    "ID_train = mean_std_feats['Id']\n",
    "ID_list = ID_train.drop_duplicates().tolist()\n",
    "\n",
    "ID_train.size\n",
    "#ID_list.size\n",
    "X_train.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = pd.DataFrame([])\n",
    "\n",
    "#leave-one-out based on recordings (we leave out one recording as test at a time)\n",
    "for i in range(0,len(ID_list)):\n",
    "    \n",
    "    idnow = ID_list[i]\n",
    "    ID_train_list = ID_train.to_list()\n",
    "    val_index = [i for i, x in enumerate(ID_train_list) if x == idnow]\n",
    "    train_index = [i for i, x in enumerate(ID_train_list) if x != idnow]\n",
    "\n",
    "    X_train1, X_val1 = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train1, y_val1 = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "\n",
    "    #normalize train set\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train1)\n",
    "    X_trainNorm1 = scaler.transform(X_train1.values)\n",
    "    X_valNorm1 = scaler.transform(X_val1.values)\n",
    "\n",
    "    #TODO: optimize the penaly weight\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "    logReg = SGDClassifier(loss='log', penalty='elasticnet')\n",
    "    logReg.fit(X_trainNorm1, y_train1)\n",
    "    y_hat_prob = logReg.predict_proba(X_valNorm1)\n",
    "    classes =logReg.classes_\n",
    "    pred_probs = pred_probs.append(pd.DataFrame({'ID': ID_train[val_index], str(classes[0]): y_hat_prob[:,0],\n",
    "                                                 str(classes[1]): y_hat_prob[:,1]}),ignore_index=True, sort=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(prob_dry,prob_wet):\n",
    "    if prob_dry > prob_wet :\n",
    "        return 'Dry'\n",
    "    else:\n",
    "        return 'Wet'\n",
    "\n",
    "#get probability per recording\n",
    "def get_predClass_per_audio(pred_probs, label_dict):\n",
    "\n",
    "    mean_pred_probs = pred_probs.groupby('ID').aggregate('mean').reset_index()\n",
    "\n",
    "    mean_pred_probs['pred_class'] = mean_pred_probs.apply(lambda x: predict_class(x['Dry'], x['Wet']), axis=1)\n",
    "    \n",
    "    #add actual classes\n",
    "    mean_pred_probs['label'] = mean_pred_probs[\"ID\"].map(label_dict)\n",
    "    return mean_pred_probs\n",
    "\n",
    "mean_pred_probs = get_predClass_per_audio(pred_probs, label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "Accuracy: 0.638889\n",
      "Precision: 0.626623\n",
      "F1-score: 0.624699\n",
      "Recall: 0.623810\n",
      "\n",
      "Confusion matrix\n",
      "pred_class  Dry  Wet  All\n",
      "label                    \n",
      "Dry           8    7   15\n",
      "Wet           6   15   21\n",
      "All          14   22   36\n"
     ]
    }
   ],
   "source": [
    "import classifEvaluationFunctions as eval\n",
    "eval.evaluation_Step(mean_pred_probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1325,)\n",
      "(1325, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "X = X_trainNorm1\n",
    "X.shape\n",
    "number=LabelEncoder()\n",
    "y = number.fit_transform(y_train1.astype('string'))\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 127)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "filter_size = 2\n",
    "def build_model_graph():\n",
    "    # define the keras model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    #model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    return model\n",
    "\n",
    "model = build_model_graph()\n",
    "\n",
    "\n",
    "# Display model architecture summary \n",
    "#model.summary()\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test,y_test, verbose=0)\n",
    "accuracy = 100*score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 82.91%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "num_epochs = 200\n",
    "num_batch_size = 30\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=0)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Training Accuracy: {0:.2%}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 85.71%\n",
      "Training Accuracy: 84.96%\n",
      "Training Accuracy: 87.22%\n",
      "Training Accuracy: 84.96%\n",
      "Training Accuracy: 84.21%\n",
      "Training Accuracy: 83.46%\n",
      "Training Accuracy: 81.95%\n",
      "Training Accuracy: 81.95%\n",
      "Training Accuracy: 83.46%\n",
      "Training Accuracy: 81.95%\n",
      "Accuracy: 83.98%\n"
     ]
    }
   ],
   "source": [
    "accuracy=[]\n",
    "for i in range(0,10):\n",
    "    model = build_model_graph()\n",
    "    num_epochs = 100\n",
    "    num_batch_size = 30\n",
    "    model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=0)\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    accuracy.append(score[1])\n",
    "    print(\"Training Accuracy: {0:.2%}\".format(score[1]))\n",
    "\n",
    "mean_accuracy = np.mean(accuracy)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(mean_accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
